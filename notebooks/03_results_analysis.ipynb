{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Results Analysis\n",
    "\n",
    "This notebook provides an in-depth analysis of the model results, including:\n",
    "- Detailed performance metrics (precision, recall, F1-score)\n",
    "- ROC curves and AUC analysis\n",
    "- Occlusion sensitivity analysis\n",
    "- Statistical significance testing\n",
    "- Impact of synthetic data per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    precision_recall_fscore_support, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from src.kew_synthetic.data.loader import KewMNISTLoader\n",
    "from src.kew_synthetic.evaluation.metrics import ModelEvaluator\n",
    "from src.kew_synthetic.evaluation.occlusion import OcclusionAnalyzer\n",
    "from src.kew_synthetic.evaluation.visualization import ResultVisualizer\n",
    "from src.kew_synthetic.utils.config import load_config\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"Analysis environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data\n",
    "\n",
    "First, let's load the trained models and test data for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path(\"../configs/\")\n",
    "model_config = load_config(config_path / \"model_config.yaml\")\n",
    "training_config = load_config(config_path / \"training_config.yaml\")\n",
    "\n",
    "# Load models\n",
    "model_dir = Path(\"../models\")\n",
    "model_original = tf.keras.models.load_model(model_dir / \"kew_mnist_original.h5\")\n",
    "model_synthetic = tf.keras.models.load_model(model_dir / \"kew_mnist_synthetic.h5\")\n",
    "\n",
    "print(\"✓ Models loaded successfully!\")\n",
    "\n",
    "# Load saved metrics\n",
    "import pickle\n",
    "\n",
    "with open(model_dir / \"evaluation_metrics.pkl\", 'rb') as f:\n",
    "    saved_metrics = pickle.load(f)\n",
    "    \n",
    "with open(model_dir / \"training_histories.pkl\", 'rb') as f:\n",
    "    training_histories = pickle.load(f)\n",
    "\n",
    "print(\"✓ Saved metrics loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "data_dir = Path(\"../data\")\n",
    "loader = KewMNISTLoader(data_dir=data_dir)\n",
    "\n",
    "# Load original dataset to get test set\n",
    "(_, _), (X_test, y_test), class_names = loader.load_original_data()\n",
    "\n",
    "print(f\"✓ Test data loaded: {X_test.shape[0]} images\")\n",
    "print(f\"✓ Classes: {', '.join(class_names)}\")\n",
    "\n",
    "# Get predictions from both models\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_original = model_original.predict(X_test, verbose=0)\n",
    "y_pred_original_classes = np.argmax(y_pred_original, axis=1)\n",
    "\n",
    "y_pred_synthetic = model_synthetic.predict(X_test, verbose=0)\n",
    "y_pred_synthetic_classes = np.argmax(y_pred_synthetic, axis=1)\n",
    "\n",
    "print(\"✓ Predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision, Recall, and F1-Score Analysis\n",
    "\n",
    "Let's perform a detailed analysis of precision, recall, and F1-scores for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Get classification reports\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT - ORIGINAL MODEL\")\n",
    "print(\"=\"*60)\n",
    "report_original = classification_report(y_test, y_pred_original_classes, \n",
    "                                      target_names=class_names, \n",
    "                                      output_dict=True)\n",
    "print(classification_report(y_test, y_pred_original_classes, \n",
    "                          target_names=class_names))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT - SYNTHETIC MODEL\")\n",
    "print(\"=\"*60)\n",
    "report_synthetic = classification_report(y_test, y_pred_synthetic_classes, \n",
    "                                       target_names=class_names,\n",
    "                                       output_dict=True)\n",
    "print(classification_report(y_test, y_pred_synthetic_classes, \n",
    "                          target_names=class_names))\n",
    "\n",
    "# Extract metrics for visualization\n",
    "precision_orig, recall_orig, f1_orig, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_original_classes, average=None\n",
    ")\n",
    "precision_synth, recall_synth, f1_synth, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_synthetic_classes, average=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize precision, recall, and F1 scores\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "orig_values = [precision_orig, recall_orig, f1_orig]\n",
    "synth_values = [precision_synth, recall_synth, f1_synth]\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n",
    "\n",
    "for idx, (ax, metric, orig_val, synth_val) in enumerate(zip(axes, metrics, orig_values, synth_values)):\n",
    "    # Create bars\n",
    "    bars1 = ax.bar(x - width/2, orig_val, width, label='Original', color='skyblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, synth_val, width, label='Synthetic', color='lightgreen', alpha=0.8)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('Precision, Recall, and F1-Score Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display improvements\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METRIC IMPROVEMENTS (Synthetic - Original)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    prec_diff = precision_synth[i] - precision_orig[i]\n",
    "    rec_diff = recall_synth[i] - recall_orig[i]\n",
    "    f1_diff = f1_synth[i] - f1_orig[i]\n",
    "    print(f\"{class_name:<15} {prec_diff:+.4f}      {rec_diff:+.4f}      {f1_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROC Curve Analysis\n",
    "\n",
    "Let's analyze the ROC curves and AUC scores for both models to understand their discriminative ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Analysis\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Binarize the labels for multi-class ROC\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Colors for each class\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, n_classes))\n",
    "\n",
    "# Original model ROC curves\n",
    "fpr_orig = dict()\n",
    "tpr_orig = dict()\n",
    "roc_auc_orig = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr_orig[i], tpr_orig[i], _ = roc_curve(y_test_bin[:, i], y_pred_original[:, i])\n",
    "    roc_auc_orig[i] = auc(fpr_orig[i], tpr_orig[i])\n",
    "    ax1.plot(fpr_orig[i], tpr_orig[i], color=colors[i], lw=2,\n",
    "             label=f'{class_names[i]} (AUC = {roc_auc_orig[i]:.3f})')\n",
    "\n",
    "# Plot diagonal\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax1.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax1.set_title('ROC Curves - Original Model', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc=\"lower right\", fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Synthetic model ROC curves\n",
    "fpr_synth = dict()\n",
    "tpr_synth = dict()\n",
    "roc_auc_synth = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr_synth[i], tpr_synth[i], _ = roc_curve(y_test_bin[:, i], y_pred_synthetic[:, i])\n",
    "    roc_auc_synth[i] = auc(fpr_synth[i], tpr_synth[i])\n",
    "    ax2.plot(fpr_synth[i], tpr_synth[i], color=colors[i], lw=2,\n",
    "             label=f'{class_names[i]} (AUC = {roc_auc_synth[i]:.3f})')\n",
    "\n",
    "# Plot diagonal\n",
    "ax2.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax2.set_title('ROC Curves - Synthetic Model', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc=\"lower right\", fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate micro-average ROC curve and ROC area\n",
    "fpr_orig[\"micro\"], tpr_orig[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_original.ravel())\n",
    "roc_auc_orig[\"micro\"] = auc(fpr_orig[\"micro\"], tpr_orig[\"micro\"])\n",
    "\n",
    "fpr_synth[\"micro\"], tpr_synth[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_synthetic.ravel())\n",
    "roc_auc_synth[\"micro\"] = auc(fpr_synth[\"micro\"], tpr_synth[\"micro\"])\n",
    "\n",
    "# Display AUC comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AUC COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Class':<15} {'Original AUC':<15} {'Synthetic AUC':<15} {'Improvement':<15}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    improvement = roc_auc_synth[i] - roc_auc_orig[i]\n",
    "    print(f\"{class_names[i]:<15} {roc_auc_orig[i]:<15.4f} {roc_auc_synth[i]:<15.4f} {improvement:+.4f}\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "improvement_micro = roc_auc_synth[\"micro\"] - roc_auc_orig[\"micro\"]\n",
    "print(f\"{'Micro-average':<15} {roc_auc_orig['micro']:<15.4f} {roc_auc_synth['micro']:<15.4f} {improvement_micro:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Occlusion Sensitivity Analysis\n",
    "\n",
    "Let's analyze what parts of images the models focus on using occlusion sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occlusion Sensitivity Analysis\n",
    "analyzer = OcclusionAnalyzer(occlusion_size=50, stride=25)\n",
    "\n",
    "# Find interesting examples - where synthetic model corrected original's mistakes\n",
    "orig_wrong = y_pred_original_classes != y_test\n",
    "synth_correct = y_pred_synthetic_classes == y_test\n",
    "interesting_mask = orig_wrong & synth_correct\n",
    "interesting_indices = np.where(interesting_mask)[0]\n",
    "\n",
    "print(f\"Found {len(interesting_indices)} cases where synthetic model corrected original's mistakes\")\n",
    "\n",
    "# Analyze one example per class (if available)\n",
    "analyzed_classes = set()\n",
    "examples_to_analyze = []\n",
    "\n",
    "for idx in interesting_indices:\n",
    "    true_class = y_test[idx]\n",
    "    if true_class not in analyzed_classes and len(examples_to_analyze) < 3:\n",
    "        analyzed_classes.add(true_class)\n",
    "        examples_to_analyze.append(idx)\n",
    "\n",
    "print(f\"\\nAnalyzing {len(examples_to_analyze)} examples...\")\n",
    "\n",
    "# Perform occlusion analysis\n",
    "fig, axes = plt.subplots(len(examples_to_analyze), 4, figsize=(16, 4*len(examples_to_analyze)))\n",
    "\n",
    "if len(examples_to_analyze) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, idx in enumerate(examples_to_analyze):\n",
    "    image = X_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Calculate occlusion maps\n",
    "    print(f\"\\nAnalyzing example {i+1}/{len(examples_to_analyze)}...\")\n",
    "    occlusion_map_orig = analyzer.generate_occlusion_map(model_original, image, true_label)\n",
    "    occlusion_map_synth = analyzer.generate_occlusion_map(model_synthetic, image, true_label)\n",
    "    \n",
    "    # Original image\n",
    "    axes[i, 0].imshow(image, cmap='gray')\n",
    "    axes[i, 0].set_title(f'Original Image\\nTrue: {class_names[true_label]}', fontsize=12)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Original model occlusion map\n",
    "    im1 = axes[i, 1].imshow(occlusion_map_orig, cmap='hot', interpolation='bilinear')\n",
    "    axes[i, 1].set_title(f'Original Model Focus\\nPred: {class_names[y_pred_original_classes[idx]]} (✗)', \n",
    "                         fontsize=12)\n",
    "    axes[i, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[i, 1], fraction=0.046)\n",
    "    \n",
    "    # Synthetic model occlusion map\n",
    "    im2 = axes[i, 2].imshow(occlusion_map_synth, cmap='hot', interpolation='bilinear')\n",
    "    axes[i, 2].set_title(f'Synthetic Model Focus\\nPred: {class_names[y_pred_synthetic_classes[idx]]} (✓)', \n",
    "                         fontsize=12)\n",
    "    axes[i, 2].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[i, 2], fraction=0.046)\n",
    "    \n",
    "    # Difference map\n",
    "    diff_map = occlusion_map_synth - occlusion_map_orig\n",
    "    im3 = axes[i, 3].imshow(diff_map, cmap='RdBu_r', interpolation='bilinear')\n",
    "    axes[i, 3].set_title('Focus Difference\\n(Synth - Orig)', fontsize=12)\n",
    "    axes[i, 3].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[i, 3], fraction=0.046)\n",
    "\n",
    "plt.suptitle('Occlusion Sensitivity Analysis: Model Focus Comparison', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Significance Testing\n",
    "\n",
    "Let's perform statistical tests to determine if the improvements are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance Testing\n",
    "from scipy import stats\n",
    "\n",
    "# McNemar's test for paired samples\n",
    "def mcnemar_test(y_true, y_pred1, y_pred2):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test to compare two classifiers.\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    correct1_correct2 = np.sum((y_pred1 == y_true) & (y_pred2 == y_true))\n",
    "    correct1_wrong2 = np.sum((y_pred1 == y_true) & (y_pred2 != y_true))\n",
    "    wrong1_correct2 = np.sum((y_pred1 != y_true) & (y_pred2 == y_true))\n",
    "    wrong1_wrong2 = np.sum((y_pred1 != y_true) & (y_pred2 != y_true))\n",
    "    \n",
    "    # McNemar's test statistic\n",
    "    n12 = correct1_wrong2\n",
    "    n21 = wrong1_correct2\n",
    "    \n",
    "    # Calculate test statistic and p-value\n",
    "    if n12 + n21 > 0:\n",
    "        statistic = (abs(n12 - n21) - 1)**2 / (n12 + n21)\n",
    "        p_value = 1 - stats.chi2.cdf(statistic, df=1)\n",
    "    else:\n",
    "        statistic = 0\n",
    "        p_value = 1.0\n",
    "    \n",
    "    return statistic, p_value, n12, n21\n",
    "\n",
    "# Perform McNemar's test\n",
    "statistic, p_value, n12, n21 = mcnemar_test(y_test, y_pred_original_classes, y_pred_synthetic_classes)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"McNEMAR'S TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test statistic: {statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.6f}\")\n",
    "print(f\"Original correct, Synthetic wrong: {n12}\")\n",
    "print(f\"Original wrong, Synthetic correct: {n21}\")\n",
    "print(f\"Net improvement: {n21 - n12} predictions\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\n✓ The improvement is statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(\"\\n✗ The improvement is not statistically significant (p >= 0.05)\")\n",
    "\n",
    "# Per-class statistical analysis using binomial test\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PER-CLASS SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Class':<15} {'Orig Acc':<10} {'Synth Acc':<10} {'P-value':<10} {'Significant':<12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    # Get predictions for this class\n",
    "    class_mask = y_test == i\n",
    "    if np.sum(class_mask) == 0:\n",
    "        continue\n",
    "        \n",
    "    orig_correct = np.sum((y_pred_original_classes[class_mask] == i))\n",
    "    synth_correct = np.sum((y_pred_synthetic_classes[class_mask] == i))\n",
    "    total = np.sum(class_mask)\n",
    "    \n",
    "    # Binomial test\n",
    "    if orig_correct != synth_correct:\n",
    "        p_value = stats.binom_test(synth_correct, total, orig_correct/total, alternative='two-sided')\n",
    "    else:\n",
    "        p_value = 1.0\n",
    "    \n",
    "    orig_acc = orig_correct / total\n",
    "    synth_acc = synth_correct / total\n",
    "    significant = \"Yes\" if p_value < 0.05 else \"No\"\n",
    "    \n",
    "    print(f\"{class_names[i]:<15} {orig_acc:<10.4f} {synth_acc:<10.4f} {p_value:<10.4f} {significant:<12}\")\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BOOTSTRAP CONFIDENCE INTERVALS (95%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def bootstrap_accuracy(y_true, y_pred, n_bootstrap=1000):\n",
    "    \"\"\"Calculate bootstrap confidence interval for accuracy.\"\"\"\n",
    "    accuracies = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        acc = np.mean(y_true[indices] == y_pred[indices])\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    # Calculate 95% confidence interval\n",
    "    lower = np.percentile(accuracies, 2.5)\n",
    "    upper = np.percentile(accuracies, 97.5)\n",
    "    mean = np.mean(accuracies)\n",
    "    \n",
    "    return mean, lower, upper\n",
    "\n",
    "# Calculate bootstrap CIs\n",
    "orig_mean, orig_lower, orig_upper = bootstrap_accuracy(y_test, y_pred_original_classes)\n",
    "synth_mean, synth_lower, synth_upper = bootstrap_accuracy(y_test, y_pred_synthetic_classes)\n",
    "\n",
    "print(f\"Original Model:  {orig_mean:.4f} [{orig_lower:.4f}, {orig_upper:.4f}]\")\n",
    "print(f\"Synthetic Model: {synth_mean:.4f} [{synth_lower:.4f}, {synth_upper:.4f}]\")\n",
    "\n",
    "# Check if confidence intervals overlap\n",
    "if orig_upper < synth_lower:\n",
    "    print(\"\\n✓ Confidence intervals do not overlap - strong evidence of improvement\")\n",
    "elif orig_lower > synth_upper:\n",
    "    print(\"\\n✗ Original model significantly better than synthetic model\")\n",
    "else:\n",
    "    print(\"\\n⚠ Confidence intervals overlap - improvement may not be significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Impact of Synthetic Data Per Class\n",
    "\n",
    "Let's analyze how synthetic data affected each class specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data information to analyze synthetic data distribution\n",
    "(X_train_orig, y_train_orig), _, _ = loader.load_original_data()\n",
    "(X_train_synth, y_train_synth), _, _ = loader.load_synthetic_enhanced_data()\n",
    "\n",
    "# Calculate class distributions\n",
    "orig_counts = pd.Series(y_train_orig).value_counts().sort_index()\n",
    "synth_counts = pd.Series(y_train_synth).value_counts().sort_index()\n",
    "synthetic_added = synth_counts - orig_counts\n",
    "\n",
    "# Create comprehensive analysis dataframe\n",
    "impact_analysis = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Original Count': orig_counts.values,\n",
    "    'Synthetic Added': synthetic_added.values,\n",
    "    'Total Count': synth_counts.values,\n",
    "    'Increase %': (synthetic_added.values / orig_counts.values * 100),\n",
    "    'Original Accuracy': [report_original[cn]['recall'] for cn in class_names],\n",
    "    'Synthetic Accuracy': [report_synthetic[cn]['recall'] for cn in class_names],\n",
    "    'Accuracy Change': [report_synthetic[cn]['recall'] - report_original[cn]['recall'] for cn in class_names],\n",
    "    'F1 Original': [report_original[cn]['f1-score'] for cn in class_names],\n",
    "    'F1 Synthetic': [report_synthetic[cn]['f1-score'] for cn in class_names],\n",
    "    'F1 Change': [report_synthetic[cn]['f1-score'] - report_original[cn]['f1-score'] for cn in class_names]\n",
    "})\n",
    "\n",
    "print(\"IMPACT OF SYNTHETIC DATA PER CLASS\")\n",
    "print(\"=\"*100)\n",
    "print(impact_analysis.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Visualize the relationship between synthetic data added and performance improvement\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Synthetic data added vs Accuracy improvement\n",
    "ax1.scatter(impact_analysis['Synthetic Added'], impact_analysis['Accuracy Change'], \n",
    "           s=100, alpha=0.7, c=range(len(class_names)), cmap='Set3')\n",
    "for i, txt in enumerate(class_names):\n",
    "    ax1.annotate(txt, (impact_analysis['Synthetic Added'].iloc[i], \n",
    "                      impact_analysis['Accuracy Change'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax1.set_xlabel('Number of Synthetic Images Added', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy Improvement', fontsize=12)\n",
    "ax1.set_title('Synthetic Data Volume vs Accuracy Improvement', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. Original class imbalance vs improvement\n",
    "ax2.scatter(impact_analysis['Original Count'], impact_analysis['Accuracy Change'],\n",
    "           s=100, alpha=0.7, c=range(len(class_names)), cmap='Set3')\n",
    "for i, txt in enumerate(class_names):\n",
    "    ax2.annotate(txt, (impact_analysis['Original Count'].iloc[i], \n",
    "                      impact_analysis['Accuracy Change'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax2.set_xlabel('Original Training Samples', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy Improvement', fontsize=12)\n",
    "ax2.set_title('Original Class Size vs Improvement', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 3. Percentage increase vs F1 improvement\n",
    "ax3.scatter(impact_analysis['Increase %'], impact_analysis['F1 Change'],\n",
    "           s=100, alpha=0.7, c=range(len(class_names)), cmap='Set3')\n",
    "for i, txt in enumerate(class_names):\n",
    "    ax3.annotate(txt, (impact_analysis['Increase %'].iloc[i], \n",
    "                      impact_analysis['F1 Change'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax3.set_xlabel('Percentage Increase in Training Data', fontsize=12)\n",
    "ax3.set_ylabel('F1-Score Improvement', fontsize=12)\n",
    "ax3.set_title('Data Increase Percentage vs F1 Improvement', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Bar chart of net improvement\n",
    "colors = ['green' if x > 0 else 'red' for x in impact_analysis['Accuracy Change']]\n",
    "bars = ax4.bar(class_names, impact_analysis['Accuracy Change'], color=colors, alpha=0.7)\n",
    "ax4.set_xlabel('Class', fontsize=12)\n",
    "ax4.set_ylabel('Accuracy Change', fontsize=12)\n",
    "ax4.set_title('Net Accuracy Change by Class', fontsize=14, fontweight='bold')\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val, synth_count in zip(bars, impact_analysis['Accuracy Change'], impact_analysis['Synthetic Added']):\n",
    "    ax4.annotate(f'{val:.3f}\\n(+{int(synth_count)})',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, val),\n",
    "                xytext=(0, 3 if val >= 0 else -20),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom' if val >= 0 else 'top', fontsize=9)\n",
    "\n",
    "plt.suptitle('Impact of Synthetic Data on Model Performance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "correlations = {\n",
    "    'Synthetic Added vs Accuracy Change': impact_analysis['Synthetic Added'].corr(impact_analysis['Accuracy Change']),\n",
    "    'Original Count vs Accuracy Change': impact_analysis['Original Count'].corr(impact_analysis['Accuracy Change']),\n",
    "    'Increase % vs F1 Change': impact_analysis['Increase %'].corr(impact_analysis['F1 Change'])\n",
    "}\n",
    "\n",
    "for metric, corr in correlations.items():\n",
    "    print(f\"{metric}: {corr:.4f}\")\n",
    "    \n",
    "# Identify most and least benefited classes\n",
    "most_improved = impact_analysis.nlargest(3, 'Accuracy Change')\n",
    "least_improved = impact_analysis.nsmallest(3, 'Accuracy Change')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSES THAT BENEFITED MOST:\")\n",
    "print(\"=\"*50)\n",
    "for _, row in most_improved.iterrows():\n",
    "    print(f\"{row['Class']}: +{row['Accuracy Change']:.4f} accuracy \"\n",
    "          f\"(added {int(row['Synthetic Added'])} synthetic images)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSES THAT BENEFITED LEAST:\")\n",
    "print(\"=\"*50)\n",
    "for _, row in least_improved.iterrows():\n",
    "    print(f\"{row['Class']}: {row['Accuracy Change']:.4f} accuracy \"\n",
    "          f\"(added {int(row['Synthetic Added'])} synthetic images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Summary and Insights\n",
    "\n",
    "Let's summarize all findings from our comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Generate comprehensive summary report\nprint(\"=\"*80)\nprint(\"COMPREHENSIVE RESULTS ANALYSIS SUMMARY\")\nprint(\"=\"*80)\n\n# 1. Overall Performance\nprint(\"\\n1. OVERALL MODEL PERFORMANCE:\")\nprint(\"-\"*40)\norig_acc = saved_metrics['original']['accuracy']\nsynth_acc = saved_metrics['synthetic']['accuracy']\nimprovement = synth_acc - orig_acc\nprint(f\"   Original Model Accuracy: {orig_acc:.4f}\")\nprint(f\"   Synthetic Model Accuracy: {synth_acc:.4f}\")\nprint(f\"   Absolute Improvement: {improvement:.4f}\")\nprint(f\"   Relative Improvement: {improvement/orig_acc*100:.2f}%\")\n\n# 2. Statistical Significance\nprint(\"\\n2. STATISTICAL SIGNIFICANCE:\")\nprint(\"-\"*40)\n# Using the McNemar's test results from earlier\nprint(f\"   McNemar's test p-value: {p_value:.6f}\")\nprint(f\"   Result: {'Statistically significant' if p_value < 0.05 else 'Not statistically significant'}\")\nprint(f\"   95% CI Original: [{orig_lower:.4f}, {orig_upper:.4f}]\")\nprint(f\"   95% CI Synthetic: [{synth_lower:.4f}, {synth_upper:.4f}]\")\n\n# 3. Metric Summary\nprint(\"\\n3. AVERAGE METRIC IMPROVEMENTS:\")\nprint(\"-\"*40)\navg_prec_improvement = np.mean(precision_synth - precision_orig)\navg_recall_improvement = np.mean(recall_synth - recall_orig)\navg_f1_improvement = np.mean(f1_synth - f1_orig)\nprint(f\"   Average Precision Improvement: {avg_prec_improvement:.4f}\")\nprint(f\"   Average Recall Improvement: {avg_recall_improvement:.4f}\")\nprint(f\"   Average F1-Score Improvement: {avg_f1_improvement:.4f}\")\n\n# 4. ROC-AUC Summary\nprint(\"\\n4. ROC-AUC ANALYSIS:\")\nprint(\"-\"*40)\navg_auc_orig = np.mean([roc_auc_orig[i] for i in range(n_classes)])\navg_auc_synth = np.mean([roc_auc_synth[i] for i in range(n_classes)])\nprint(f\"   Average AUC Original: {avg_auc_orig:.4f}\")\nprint(f\"   Average AUC Synthetic: {avg_auc_synth:.4f}\")\nprint(f\"   Average AUC Improvement: {avg_auc_synth - avg_auc_orig:.4f}\")\n\n# 5. Synthetic Data Impact\nprint(\"\\n5. SYNTHETIC DATA IMPACT:\")\nprint(\"-\"*40)\ntotal_synthetic = impact_analysis['Synthetic Added'].sum()\nprint(f\"   Total Synthetic Images Added: {total_synthetic:,}\")\nprint(f\"   Average Synthetic per Class: {total_synthetic/len(class_names):.0f}\")\nprint(f\"   Dataset Size Increase: {total_synthetic/len(y_train_orig)*100:.1f}%\")\n\n# 6. Class-Specific Insights\nprint(\"\\n6. CLASS-SPECIFIC INSIGHTS:\")\nprint(\"-\"*40)\nimproved_classes = impact_analysis[impact_analysis['Accuracy Change'] > 0]\ndegraded_classes = impact_analysis[impact_analysis['Accuracy Change'] < 0]\nprint(f\"   Classes Improved: {len(improved_classes)} / {len(class_names)}\")\nprint(f\"   Classes Degraded: {len(degraded_classes)} / {len(class_names)}\")\nprint(f\"   Most Improved: {most_improved.iloc[0]['Class']} (+{most_improved.iloc[0]['Accuracy Change']:.4f})\")\nif len(degraded_classes) > 0:\n    print(f\"   Most Degraded: {least_improved.iloc[0]['Class']} ({least_improved.iloc[0]['Accuracy Change']:.4f})\")\n\n# 7. Key Findings\nprint(\"\\n7. KEY FINDINGS:\")\nprint(\"-\"*40)\nfindings = []\n\nif improvement > 0:\n    findings.append(\"✓ Synthetic data augmentation successfully improved overall model accuracy\")\n    \nif p_value < 0.05:\n    findings.append(\"✓ The improvement is statistically significant\")\n    \nif len(improved_classes) > len(degraded_classes):\n    findings.append(\"✓ More classes benefited from synthetic data than were harmed\")\n    \nif correlations['Original Count vs Accuracy Change'] < 0:\n    findings.append(\"✓ Underrepresented classes benefited more from synthetic augmentation\")\n    \nif avg_auc_synth > avg_auc_orig:\n    findings.append(\"✓ Model's discriminative ability improved across all classes\")\n\nfor finding in findings:\n    print(f\"   {finding}\")\n\n# 8. Recommendations\nprint(\"\\n8. RECOMMENDATIONS:\")\nprint(\"-\"*40)\nrecommendations = []\n\nif len(degraded_classes) > 0:\n    recommendations.append(\"• Consider adjusting synthetic data generation for classes that degraded\")\n    \nif impact_analysis['Increase %'].std() > 50:\n    recommendations.append(\"• Consider more uniform synthetic data distribution across classes\")\n    \nrecommendations.append(\"• Continue monitoring model performance on new data\")\nrecommendations.append(\"• Consider iterative synthetic data generation based on error analysis\")\n\nfor rec in recommendations:\n    print(f\"   {rec}\")\n\n# Create final visualization summary\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# 1. Overall metrics comparison\nmetrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\norig_metrics = [saved_metrics['original']['accuracy'], \n                saved_metrics['original']['precision'],\n                saved_metrics['original']['recall'], \n                saved_metrics['original']['f1_score']]\nsynth_metrics = [saved_metrics['synthetic']['accuracy'],\n                 saved_metrics['synthetic']['precision'],\n                 saved_metrics['synthetic']['recall'],\n                 saved_metrics['synthetic']['f1_score']]\n\nx = np.arange(len(metrics_names))\nwidth = 0.35\n\nbars1 = axes[0, 0].bar(x - width/2, orig_metrics, width, label='Original', color='skyblue')\nbars2 = axes[0, 0].bar(x + width/2, synth_metrics, width, label='Synthetic', color='lightgreen')\n\naxes[0, 0].set_xlabel('Metric')\naxes[0, 0].set_ylabel('Score')\naxes[0, 0].set_title('Overall Performance Metrics')\naxes[0, 0].set_xticks(x)\naxes[0, 0].set_xticklabels(metrics_names)\naxes[0, 0].legend()\naxes[0, 0].set_ylim(0, 1)\n\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[0, 0].annotate(f'{height:.3f}',\n                          xy=(bar.get_x() + bar.get_width() / 2, height),\n                          xytext=(0, 3),\n                          textcoords=\"offset points\",\n                          ha='center', va='bottom')\n\n# 2. Class-wise improvement summary\naxes[0, 1].bar(class_names, impact_analysis['Accuracy Change'], \n              color=['green' if x > 0 else 'red' for x in impact_analysis['Accuracy Change']])\naxes[0, 1].axhline(y=0, color='black', linestyle='-')\naxes[0, 1].set_xlabel('Class')\naxes[0, 1].set_ylabel('Accuracy Change')\naxes[0, 1].set_title('Performance Change by Class')\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# 3. Training data distribution\ntrain_data = pd.DataFrame({\n    'Original': orig_counts.values,\n    'Synthetic Added': synthetic_added.values\n})\ntrain_data.plot(kind='bar', ax=axes[1, 0], color=['skyblue', 'lightgreen'])\naxes[1, 0].set_xlabel('Class')\naxes[1, 0].set_ylabel('Number of Images')\naxes[1, 0].set_title('Training Data Distribution')\naxes[1, 0].set_xticklabels(class_names, rotation=45)\naxes[1, 0].legend()\n\n# 4. F1-Score comparison\naxes[1, 1].plot(class_names, impact_analysis['F1 Original'], 'o-', label='Original', color='blue')\naxes[1, 1].plot(class_names, impact_analysis['F1 Synthetic'], 'o-', label='Synthetic', color='green')\naxes[1, 1].set_xlabel('Class')\naxes[1, 1].set_ylabel('F1-Score')\naxes[1, 1].set_title('F1-Score Comparison')\naxes[1, 1].tick_params(axis='x', rotation=45)\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.suptitle('Comprehensive Results Summary', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Analysis complete! Results have been thoroughly evaluated.\")\nprint(\"=\"*80)",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}